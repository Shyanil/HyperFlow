{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "### Code Explanation\n",
    "\n",
    "This Python script demonstrates the use of the `FlowUnit` class from the `FlowUnit_module` to compute the dot product of two `FlowUnit` objects and measure the execution time of the operation. Here's a step-by-step breakdown of what each part of the code does:\n",
    "\n",
    "1. **Importing Required Modules**:\n",
    "   ```python\n",
    "   import time\n",
    "   from FlowUnit_module import FlowUnit\n",
    "   ```\n",
    "   - `time`: The `time` module is imported to measure the execution time of the dot product computation.\n",
    "   - `FlowUnit`: The `FlowUnit` class is imported from the `FlowUnit_module`. This class is likely a custom class designed for handling data as \"flow units\" and providing certain methods, such as the `__dot__` method used for computing the dot product.\n",
    "\n",
    "2. **Creating FlowUnit Objects**:\n",
    "   ```python\n",
    "   a = FlowUnit([1, 2, 3])\n",
    "   b = FlowUnit([4, 5, 6])\n",
    "   ```\n",
    "   - Two instances of the `FlowUnit` class are created, `a` and `b`, with input data `[1, 2, 3]` and `[4, 5, 6]`, respectively. These instances presumably wrap the input data into a specialized structure for processing.\n",
    "\n",
    "3. **Computing the Dot Product**:\n",
    "   ```python\n",
    "   result = a.__dot__(b)\n",
    "   ```\n",
    "   - The `__dot__` method is called on the `FlowUnit` object `a`, passing `b` as an argument. This method is likely responsible for computing the dot product of the two vectors (data held in `a` and `b`). \n",
    "   - The result of the dot product is stored in the variable `result`. This result is expected to be another `FlowUnit` object, which contains the computed dot product value.\n",
    "\n",
    "4. **Printing the Results**:\n",
    "   ```python\n",
    "   start_time = time.time()\n",
    "   print(f\"Dot product result: {result.data}\")\n",
    "   ```\n",
    "   - The current time is recorded using `time.time()` to track the start time for the dot product operation.\n",
    "   - The result of the dot product, which is stored in `result.data`, is printed. The `data` attribute is assumed to hold the computed value of the dot product.\n",
    "\n",
    "5. **Measuring Execution Time**:\n",
    "   ```python\n",
    "   end_time = time.time()\n",
    "   py_time = end_time - start_time\n",
    "   print(f\"Time taken using NumPy: {py_time:.6f} seconds\")\n",
    "   ```\n",
    "   - The time is recorded again using `time.time()` after the print statement.\n",
    "   - The time taken to compute the dot product is calculated by subtracting the start time from the end time (`py_time = end_time - start_time`).\n",
    "   - The execution time is printed, formatted to six decimal places, showing the time taken to compute the dot product.\n",
    "\n",
    "### Summary\n",
    "This code demonstrates the calculation of the dot product of two vectors wrapped in `FlowUnit` objects. The time taken for the computation is measured and displayed in seconds, providing insight into the performance of the operation.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot product result: 32\n",
      "Time taken using NumPy: 0.000000 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from FlowUnit_module import FlowUnit\n",
    "a = FlowUnit([1, 2, 3])\n",
    "b = FlowUnit([4, 5, 6])\n",
    "\n",
    "# Compute the dot product\n",
    "result = a.__dot__(b)\n",
    "\n",
    "# Print results\n",
    "start_time = time.time()\n",
    "print(f\"Dot product result: {result.data}\")\n",
    "end_time = time.time()\n",
    "py_time = end_time - start_time\n",
    "print(f\"Time taken using NumPy: {py_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "### Code Explanation\n",
    "\n",
    "This Python code defines a test function `test_create2d_array()` that checks if the `FlowUnit.create2d_array()` function produces the same output as NumPy's `np.linspace()` method when creating a 2D array. Here's a step-by-step breakdown of what each part of the code does:\n",
    "\n",
    "1. **Importing the NumPy Module**:\n",
    "   ```python\n",
    "   import numpy as np\n",
    "   ```\n",
    "   - The `numpy` module is imported with the alias `np`, which is a powerful library for numerical operations in Python.\n",
    "\n",
    "2. **Defining the Test Function**:\n",
    "   ```python\n",
    "   def test_create2d_array():\n",
    "   ```\n",
    "   - A function `test_create2d_array()` is defined to test the creation of a 2D array using both a custom method (`FlowUnit.create2d_array()`) and NumPy's built-in method.\n",
    "\n",
    "3. **Initializing Parameters**:\n",
    "   ```python\n",
    "   start, end, rows, cols = 1, 10, 3, 3\n",
    "   ```\n",
    "   - The test function initializes variables:\n",
    "     - `start`: The starting value for the range (1).\n",
    "     - `end`: The ending value for the range (10).\n",
    "     - `rows`: The number of rows in the resulting 2D array (3).\n",
    "     - `cols`: The number of columns in the resulting 2D array (3).\n",
    "\n",
    "4. **Creating the 2D Array Using FlowUnit**:\n",
    "   ```python\n",
    "   flow_instance = FlowUnit.create2d_array(start, end, rows, cols)\n",
    "   flow_output = flow_instance.data \n",
    "   ```\n",
    "   - The `create2d_array()` function of `FlowUnit` is called to create a 2D array. It takes the `start`, `end`, `rows`, and `cols` as inputs. \n",
    "   - The output is stored in `flow_instance`, and the actual array is accessed via `flow_instance.data`. This assumes that `FlowUnit` objects have a `data` attribute that holds the array.\n",
    "\n",
    "5. **Creating the 2D Array Using NumPy**:\n",
    "   ```python\n",
    "   np_output = np.linspace(start, end, rows * cols).reshape(rows, cols)\n",
    "   ```\n",
    "   - The `np.linspace()` function is used to generate `rows * cols` equally spaced values between `start` and `end` (1 and 10, respectively). \n",
    "   - These values are then reshaped into a 2D array of shape `(rows, cols)` using `.reshape(rows, cols)`.\n",
    "\n",
    "6. **Comparing the Two Outputs**:\n",
    "   ```python\n",
    "   assert np.allclose(flow_output, np_output), \"Outputs do not match!\"\n",
    "   ```\n",
    "   - The `np.allclose()` function is used to check if all elements of `flow_output` and `np_output` are numerically equal, with a tolerance for floating-point differences.\n",
    "   - If the arrays do not match, an assertion error will be raised with the message `\"Outputs do not match!\"`.\n",
    "\n",
    "7. **Printing the Test Result**:\n",
    "   ```python\n",
    "   print(\"Test passed! Both outputs are the same.\")\n",
    "   ```\n",
    "   - If the assertion passes, meaning the arrays are identical, the message `\"Test passed! Both outputs are the same.\"` is printed.\n",
    "\n",
    "8. **Running the Test**:\n",
    "   ```python\n",
    "   test_create2d_array()\n",
    "   ```\n",
    "   - Finally, the `test_create2d_array()` function is called to run the test.\n",
    "\n",
    "### Summary\n",
    "This code defines and runs a test that compares the output of the `FlowUnit.create2d_array()` function with the output of NumPy's `np.linspace()` method, both generating a 3x3 2D array. If the outputs match, a success message is printed. If they do not match, an assertion error is raised.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed! Both outputs are the same.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def test_create2d_array():\n",
    "    start, end, rows, cols = 1, 10, 3, 3\n",
    "    \n",
    "    # Using FlowUnit's function\n",
    "    flow_instance = FlowUnit.create2d_array(start, end, rows, cols)\n",
    "    flow_output = flow_instance.data  # Assuming data attribute exists\n",
    "    \n",
    "    # Using NumPy's linspace\n",
    "    np_output = np.linspace(start, end, rows * cols).reshape(rows, cols)\n",
    "    \n",
    "    # Check if both outputs match\n",
    "    assert np.allclose(flow_output, np_output), \"Outputs do not match!\"\n",
    "    print(\"Test passed! Both outputs are the same.\")\n",
    "\n",
    "# Run the test\n",
    "test_create2d_array()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "### Code Explanation\n",
    "\n",
    "This Python code defines a test function `test_convert2d_array()` to check if the `FlowUnit.convert2d_array()` function correctly converts a list of values into a 2D array that matches the output of NumPy's `reshape()` method. Here's a step-by-step breakdown of what each part of the code does:\n",
    "\n",
    "1. **Importing the NumPy Module**:\n",
    "   ```python\n",
    "   import numpy as np\n",
    "   ```\n",
    "   - The `numpy` module is imported with the alias `np`. NumPy is a library for numerical computing, commonly used for handling arrays and matrix operations.\n",
    "\n",
    "2. **Defining the Test Function**:\n",
    "   ```python\n",
    "   def test_convert2d_array():\n",
    "   ```\n",
    "   - A function `test_convert2d_array()` is defined to test the conversion of a 1D list into a 2D array.\n",
    "\n",
    "3. **Initializing Input Values**:\n",
    "   ```python\n",
    "   values = list(range(1, 10))  # Example input values\n",
    "   rows, cols = 3, 3\n",
    "   ```\n",
    "   - A list `values` is created containing the integers from 1 to 9 (`range(1, 10)`), which will serve as the input data for conversion into a 2D array.\n",
    "   - The number of `rows` and `cols` for the 2D array is set to 3, making a 3x3 array.\n",
    "\n",
    "4. **Converting the List to a 2D Array Using FlowUnit**:\n",
    "   ```python\n",
    "   flow_instance = FlowUnit.convert2d_array(values, rows, cols)\n",
    "   flow_output = flow_instance.data  # Assuming data attribute exists\n",
    "   ```\n",
    "   - The `convert2d_array()` function of `FlowUnit` is called with `values`, `rows`, and `cols` as inputs. This function is expected to convert the input list into a 2D array.\n",
    "   - The output is stored in `flow_instance`, and the actual array is accessed via `flow_instance.data`. This assumes that the `FlowUnit` object has a `data` attribute that holds the array.\n",
    "\n",
    "5. **Converting the List to a 2D Array Using NumPy**:\n",
    "   ```python\n",
    "   np_output = np.array(values).reshape(rows, cols)\n",
    "   ```\n",
    "   - The `np.array()` function is used to convert the `values` list into a NumPy array. \n",
    "   - The `reshape(rows, cols)` method is then applied to reshape the array into a 3x3 2D array.\n",
    "\n",
    "6. **Comparing the Two Outputs**:\n",
    "   ```python\n",
    "   assert np.array_equal(flow_output, np_output), \"Outputs do not match!\"\n",
    "   ```\n",
    "   - The `np.array_equal()` function checks if the two arrays `flow_output` and `np_output` are exactly equal in terms of shape and values.\n",
    "   - If the arrays do not match, an assertion error will be raised with the message `\"Outputs do not match!\"`.\n",
    "\n",
    "7. **Printing the Test Result**:\n",
    "   ```python\n",
    "   print(\"Test passed! Both outputs are the same.\")\n",
    "   ```\n",
    "   - If the assertion passes, meaning the arrays are identical, the message `\"Test passed! Both outputs are the same.\"` is printed.\n",
    "\n",
    "8. **Running the Test**:\n",
    "   ```python\n",
    "   test_convert2d_array()\n",
    "   ```\n",
    "   - Finally, the `test_convert2d_array()` function is called to run the test.\n",
    "\n",
    "### Summary\n",
    "This code defines and runs a test that compares the output of the `FlowUnit.convert2d_array()` function with the output of NumPy's `reshape()` method. Both methods are used to convert a 1D list into a 3x3 2D array. If the outputs match, a success message is printed. If they do not match, an assertion error is raised.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed! Both outputs are the same.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def test_convert2d_array():\n",
    "    values = list(range(1, 10))  # Example input values\n",
    "    rows, cols = 3, 3\n",
    "    \n",
    "    # Using FlowUnit's function\n",
    "    flow_instance = FlowUnit.convert2d_array(values, rows, cols)\n",
    "    flow_output = flow_instance.data  # Assuming data attribute exists\n",
    "    \n",
    "    # Using NumPy's reshape\n",
    "    np_output = np.array(values).reshape(rows, cols)\n",
    "    \n",
    "    # Check if both outputs match\n",
    "    assert np.array_equal(flow_output, np_output), \"Outputs do not match!\"\n",
    "    print(\"Test passed! Both outputs are the same.\")\n",
    "\n",
    "# Run the test\n",
    "test_convert2d_array()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "### Code Explanation\n",
    "\n",
    "This Python code defines a test function `test_flowunit_addition()` that verifies if the addition operation of two `FlowUnit` instances works as expected by comparing it to NumPy's addition operation. Here's a step-by-step breakdown of what each part of the code does:\n",
    "\n",
    "1. **Importing the NumPy Module**:\n",
    "   ```python\n",
    "   import numpy as np\n",
    "   ```\n",
    "   - The `numpy` module is imported with the alias `np`. NumPy is a library used for numerical computing, particularly for handling arrays and performing matrix operations.\n",
    "\n",
    "2. **Defining the Test Function**:\n",
    "   ```python\n",
    "   def test_flowunit_addition():\n",
    "   ```\n",
    "   - A function `test_flowunit_addition()` is defined to test the addition operation of `FlowUnit` instances.\n",
    "\n",
    "3. **Creating Sample Data**:\n",
    "   ```python\n",
    "   data1 = np.array([[1, 2], [3, 4]])\n",
    "   data2 = np.array([[5, 6], [7, 8]])\n",
    "   ```\n",
    "   - Two 2x2 NumPy arrays `data1` and `data2` are created. These arrays will be used as the input data for the `FlowUnit` instances.\n",
    "\n",
    "4. **Creating FlowUnit Instances**:\n",
    "   ```python\n",
    "   flow1 = FlowUnit(data1)\n",
    "   flow2 = FlowUnit(data2)\n",
    "   ```\n",
    "   - Two `FlowUnit` instances, `flow1` and `flow2`, are created with the data `data1` and `data2`, respectively.\n",
    "\n",
    "5. **Using FlowUnit's Addition Operation**:\n",
    "   ```python\n",
    "   flow_output = (flow1 + flow2).data  # Assuming data attribute exists\n",
    "   ```\n",
    "   - The addition operator (`+`) is applied to the `FlowUnit` instances `flow1` and `flow2`. This assumes that the `FlowUnit` class has an overloaded `+` operator that performs element-wise addition on the `data` attribute of the instances.\n",
    "   - The resulting `FlowUnit` object is accessed via `.data` to retrieve the data after the addition.\n",
    "\n",
    "6. **Using NumPy's Addition**:\n",
    "   ```python\n",
    "   np_output = data1 + data2\n",
    "   ```\n",
    "   - The addition operator (`+`) is applied directly to the NumPy arrays `data1` and `data2`. NumPy will perform element-wise addition, resulting in a 2x2 array:\n",
    "     \\[\n",
    "     \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} + \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} = \\begin{bmatrix} 6 & 8 \\\\ 10 & 12 \\end{bmatrix}\n",
    "     \\]\n",
    "\n",
    "7. **Comparing the Two Outputs**:\n",
    "   ```python\n",
    "   assert np.array_equal(flow_output, np_output), \"Outputs do not match!\"\n",
    "   ```\n",
    "   - The `np.array_equal()` function checks if the two arrays `flow_output` (the result of `FlowUnit` addition) and `np_output` (the result of NumPy addition) are exactly equal in shape and values.\n",
    "   - If the arrays do not match, an assertion error will be raised with the message `\"Outputs do not match!\"`.\n",
    "\n",
    "8. **Printing the Test Result**:\n",
    "   ```python\n",
    "   print(\"Test passed! Both outputs are the same.\")\n",
    "   ```\n",
    "   - If the assertion passes, meaning the arrays are identical, the message `\"Test passed! Both outputs are the same.\"` is printed.\n",
    "\n",
    "9. **Running the Test**:\n",
    "   ```python\n",
    "   test_flowunit_addition()\n",
    "   ```\n",
    "   - Finally, the `test_flowunit_addition()` function is called to execute the test.\n",
    "\n",
    "### Summary\n",
    "This code defines and runs a test that compares the addition of two `FlowUnit` instances with the addition of two NumPy arrays. The test checks whether the addition operation in `FlowUnit` produces the same result as NumPy's addition. If the results match, a success message is printed; otherwise, an assertion error is raised.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed! Both outputs are the same.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def test_flowunit_addition():\n",
    "    # Create sample data\n",
    "    data1 = np.array([[1, 2], [3, 4]])\n",
    "    data2 = np.array([[5, 6], [7, 8]])\n",
    "    \n",
    "    # Creating FlowUnit instances\n",
    "    flow1 = FlowUnit(data1)\n",
    "    flow2 = FlowUnit(data2)\n",
    "    \n",
    "    # Using FlowUnit's addition\n",
    "    flow_output = (flow1 + flow2).data  # Assuming data attribute exists\n",
    "    \n",
    "    # Using NumPy's addition\n",
    "    np_output = data1 + data2\n",
    "    \n",
    "    # Check if both outputs match\n",
    "    assert np.array_equal(flow_output, np_output), \"Outputs do not match!\"\n",
    "    print(\"Test passed! Both outputs are the same.\")\n",
    "\n",
    "# Run the test\n",
    "test_flowunit_addition()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "### Code Explanation\n",
    "\n",
    "This Python code defines a test function `test_flowunit_multiplication()` to verify if the multiplication operation of two `FlowUnit` instances works as expected by comparing it to NumPy's element-wise multiplication operation. Here's a detailed step-by-step breakdown of the code:\n",
    "\n",
    "1. **Importing the NumPy Module**:\n",
    "   ```python\n",
    "   import numpy as np\n",
    "   ```\n",
    "   - The `numpy` module is imported as `np`, which is a library widely used for numerical computations in Python, particularly for handling arrays and performing mathematical operations on them.\n",
    "\n",
    "2. **Defining the Test Function**:\n",
    "   ```python\n",
    "   def test_flowunit_multiplication():\n",
    "   ```\n",
    "   - The function `test_flowunit_multiplication()` is defined to test the multiplication functionality of `FlowUnit` instances.\n",
    "\n",
    "3. **Creating Sample Data**:\n",
    "   ```python\n",
    "   data1 = np.array([[1, 2], [3, 4]])\n",
    "   data2 = np.array([[5, 6], [7, 8]])\n",
    "   ```\n",
    "   - Two 2x2 NumPy arrays `data1` and `data2` are created. These arrays contain sample values that will be used as inputs for the `FlowUnit` instances. `data1` is a matrix:\n",
    "     \\[\n",
    "     \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\n",
    "     \\]\n",
    "     and `data2` is a matrix:\n",
    "     \\[\n",
    "     \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix}\n",
    "     \\]\n",
    "\n",
    "4. **Creating FlowUnit Instances**:\n",
    "   ```python\n",
    "   flow1 = FlowUnit(data1)\n",
    "   flow2 = FlowUnit(data2)\n",
    "   ```\n",
    "   - Two instances of `FlowUnit`, `flow1` and `flow2`, are created with `data1` and `data2` as input data, respectively. These instances presumably represent some custom objects that hold data.\n",
    "\n",
    "5. **Using FlowUnit's Multiplication Operation**:\n",
    "   ```python\n",
    "   flow_output = (flow1 * flow2).data  # Assuming data attribute exists\n",
    "   ```\n",
    "   - The multiplication operator (`*`) is applied to the `FlowUnit` instances `flow1` and `flow2`. This assumes that the `FlowUnit` class has overloaded the `*` operator to perform element-wise multiplication of the `data` attributes of the two instances.\n",
    "   - The result of the multiplication is then accessed via `.data` to retrieve the data inside the resulting `FlowUnit` object.\n",
    "\n",
    "6. **Using NumPy's Multiplication**:\n",
    "   ```python\n",
    "   np_output = data1 * data2  # Element-wise multiplication\n",
    "   ```\n",
    "   - The multiplication operator (`*`) is applied directly to the NumPy arrays `data1` and `data2`. This operation performs element-wise multiplication:\n",
    "     \\[\n",
    "     \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} * \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} = \\begin{bmatrix} 5 & 12 \\\\ 21 & 32 \\end{bmatrix}\n",
    "     \\]\n",
    "   - The resulting matrix is stored in `np_output`.\n",
    "\n",
    "7. **Comparing the Two Outputs**:\n",
    "   ```python\n",
    "   assert np.array_equal(flow_output, np_output), \"Outputs do not match!\"\n",
    "   ```\n",
    "   - The `np.array_equal()` function is used to compare the two arrays, `flow_output` (the result from the `FlowUnit` multiplication) and `np_output` (the result from NumPy's multiplication).\n",
    "   - If the arrays do not match, an assertion error is raised with the message `\"Outputs do not match!\"`.\n",
    "\n",
    "8. **Printing the Test Result**:\n",
    "   ```python\n",
    "   print(\"Test passed! Both outputs are the same.\")\n",
    "   ```\n",
    "   - If the assertion passes (i.e., the arrays are identical), the message `\"Test passed! Both outputs are the same.\"` is printed, indicating that the `FlowUnit` multiplication operation works as expected.\n",
    "\n",
    "9. **Running the Test**:\n",
    "   ```python\n",
    "   test_flowunit_multiplication()\n",
    "   ```\n",
    "   - The test function `test_flowunit_multiplication()` is called to run the test.\n",
    "\n",
    "### Summary\n",
    "This code defines and runs a test to verify if the multiplication operation on two `FlowUnit` instances produces the same result as NumPy's element-wise multiplication. If both outputs match, a success message is printed; otherwise, an assertion error is raised.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed! Both outputs are the same.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def test_flowunit_multiplication():\n",
    "    # Create sample data\n",
    "    data1 = np.array([[1, 2], [3, 4]])\n",
    "    data2 = np.array([[5, 6], [7, 8]])\n",
    "    \n",
    "    # Creating FlowUnit instances\n",
    "    flow1 = FlowUnit(data1)\n",
    "    flow2 = FlowUnit(data2)\n",
    "    \n",
    "    # Using FlowUnit's multiplication\n",
    "    flow_output = (flow1 * flow2).data  # Assuming data attribute exists\n",
    "    \n",
    "    # Using NumPy's multiplication\n",
    "    np_output = data1 * data2  # Element-wise multiplication\n",
    "    \n",
    "    # Check if both outputs match\n",
    "    assert np.array_equal(flow_output, np_output), \"Outputs do not match!\"\n",
    "    print(\"Test passed! Both outputs are the same.\")\n",
    "\n",
    "# Run the test\n",
    "test_flowunit_multiplication()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "### Code Explanation\n",
    "\n",
    "This Python code defines a test function `test_flowunit_matmul()` to verify if the matrix multiplication operation of two `FlowUnit` instances works as expected by comparing it to NumPy's matrix multiplication operation. Here's a detailed step-by-step breakdown of the code:\n",
    "\n",
    "1. **Importing the NumPy Module**:\n",
    "   ```python\n",
    "   import numpy as np\n",
    "   ```\n",
    "   - The `numpy` module is imported as `np`, which is a widely used library for numerical computations in Python, particularly for working with arrays and performing mathematical operations on them.\n",
    "\n",
    "2. **Defining the Test Function**:\n",
    "   ```python\n",
    "   def test_flowunit_matmul():\n",
    "   ```\n",
    "   - The function `test_flowunit_matmul()` is defined to test the matrix multiplication functionality of `FlowUnit` instances.\n",
    "\n",
    "3. **Creating Sample Data**:\n",
    "   ```python\n",
    "   data1 = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "   data2 = np.array([[7, 8], [9, 10], [11, 12]])\n",
    "   ```\n",
    "   - Two 2D NumPy arrays `data1` and `data2` are created. `data1` is a 2x3 matrix:\n",
    "     \\[\n",
    "     \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}\n",
    "     \\]\n",
    "     and `data2` is a 3x2 matrix:\n",
    "     \\[\n",
    "     \\begin{bmatrix} 7 & 8 \\\\ 9 & 10 \\\\ 11 & 12 \\end{bmatrix}\n",
    "     \\]\n",
    "   - These matrices are used as sample input data for the `FlowUnit` instances.\n",
    "\n",
    "4. **Creating FlowUnit Instances**:\n",
    "   ```python\n",
    "   flow1 = FlowUnit(data1)\n",
    "   flow2 = FlowUnit(data2)\n",
    "   ```\n",
    "   - Two instances of `FlowUnit`, `flow1` and `flow2`, are created using `data1` and `data2` as input data, respectively. These instances are assumed to be custom objects designed to hold the provided data.\n",
    "\n",
    "5. **Using FlowUnit's Matrix Multiplication**:\n",
    "   ```python\n",
    "   flow_output = (flow1._matmul(flow2)).data  \n",
    "   ```\n",
    "   - The matrix multiplication operator (via the `_matmul` method) is applied to the `FlowUnit` instances `flow1` and `flow2`. This assumes that the `FlowUnit` class has a method `_matmul` implemented to handle matrix multiplication for its data attribute.\n",
    "   - The result of the matrix multiplication is accessed via `.data` to retrieve the resulting data from the `FlowUnit` object.\n",
    "\n",
    "6. **Using NumPy's Matrix Multiplication**:\n",
    "   ```python\n",
    "   np_output = data1 @ data2  # Equivalent to np.matmul(data1, data2)\n",
    "   ```\n",
    "   - The `@` operator is used for matrix multiplication in NumPy. This operation is equivalent to `np.matmul(data1, data2)`. It performs matrix multiplication between `data1` (a 2x3 matrix) and `data2` (a 3x2 matrix), resulting in a 2x2 matrix:\n",
    "     \\[\n",
    "     \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix} \\times \\begin{bmatrix} 7 & 8 \\\\ 9 & 10 \\\\ 11 & 12 \\end{bmatrix} = \\begin{bmatrix} 58 & 64 \\\\ 139 & 154 \\end{bmatrix}\n",
    "     \\]\n",
    "   - The result of the multiplication is stored in `np_output`.\n",
    "\n",
    "7. **Comparing the Two Outputs**:\n",
    "   ```python\n",
    "   assert np.array_equal(flow_output, np_output), \"Outputs do not match!\"\n",
    "   ```\n",
    "   - The `np.array_equal()` function is used to compare the two arrays, `flow_output` (the result from the `FlowUnit` matrix multiplication) and `np_output` (the result from NumPy's matrix multiplication).\n",
    "   - If the arrays do not match, an assertion error is raised with the message `\"Outputs do not match!\"`.\n",
    "\n",
    "8. **Printing the Test Result**:\n",
    "   ```python\n",
    "   print(\"Test passed! Both outputs are the same.\")\n",
    "   ```\n",
    "   - If the assertion passes (i.e., the matrices are identical), the message `\"Test passed! Both outputs are the same.\"` is printed, indicating that the `FlowUnit` matrix multiplication works as expected.\n",
    "\n",
    "9. **Running the Test**:\n",
    "   ```python\n",
    "   test_flowunit_matmul()\n",
    "   ```\n",
    "   - The test function `test_flowunit_matmul()` is called to run the test.\n",
    "\n",
    "### Summary\n",
    "This code defines and runs a test to verify if the matrix multiplication operation on two `FlowUnit` instances produces the same result as NumPy's matrix multiplication. If both outputs match, a success message is printed; otherwise, an assertion error is raised.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed! Both outputs are the same.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def test_flowunit_matmul():\n",
    "    # Create sample data\n",
    "    data1 = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "    data2 = np.array([[7, 8], [9, 10], [11, 12]])\n",
    "    \n",
    "    # Creating FlowUnit instances\n",
    "    flow1 = FlowUnit(data1)\n",
    "    flow2 = FlowUnit(data2)\n",
    "    \n",
    "    # Using FlowUnit's matrix multiplication\n",
    "    flow_output = (flow1 @flow2) .data  \n",
    "    # Using NumPy's matrix multiplication\n",
    "    np_output = data1 @ data2  # Equivalent to np.matmul(data1, data2)\n",
    "    \n",
    "    # Check if both outputs match\n",
    "    assert np.array_equal(flow_output, np_output), \"Outputs do not match!\"\n",
    "    print(\"Test passed! Both outputs are the same.\")\n",
    "\n",
    "# Run the test\n",
    "test_flowunit_matmul()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "### Code Explanation\n",
    "\n",
    "This Python code defines a test function `test_flowunit_dot_product()` to verify if the dot product operation of two `FlowUnit` instances works correctly by comparing it to NumPy's `dot` product. Here's a detailed step-by-step breakdown of the code:\n",
    "\n",
    "1. **Importing the NumPy Module**:\n",
    "   ```python\n",
    "   import numpy as np\n",
    "   ```\n",
    "   - The `numpy` module is imported as `np`. This library is used for numerical computations and provides a powerful array structure, along with operations like dot product.\n",
    "\n",
    "2. **Defining the Test Function**:\n",
    "   ```python\n",
    "   def test_flowunit_dot_product():\n",
    "   ```\n",
    "   - The function `test_flowunit_dot_product()` is defined to test the dot product functionality of `FlowUnit` instances.\n",
    "\n",
    "3. **Creating Sample Data**:\n",
    "   ```python\n",
    "   data1 = np.array([1, 2, 3])\n",
    "   data2 = np.array([4, 5, 6])\n",
    "   ```\n",
    "   - Two 1D NumPy arrays `data1` and `data2` are created. These arrays represent the vectors for which the dot product will be computed.\n",
    "     \\[\n",
    "     \\text{data1} = \\begin{bmatrix} 1 & 2 & 3 \\end{bmatrix}, \\quad \\text{data2} = \\begin{bmatrix} 4 & 5 & 6 \\end{bmatrix}\n",
    "     \\]\n",
    "\n",
    "4. **Creating FlowUnit Instances**:\n",
    "   ```python\n",
    "   flow1 = FlowUnit(data1)\n",
    "   flow2 = FlowUnit(data2)\n",
    "   ```\n",
    "   - Two instances of `FlowUnit`, `flow1` and `flow2`, are created using `data1` and `data2` as input data, respectively. These instances are assumed to hold the provided data and are expected to have a method or functionality to perform dot product operations.\n",
    "\n",
    "5. **Using FlowUnit's Dot Product**:\n",
    "   ```python\n",
    "   flow_output = flow1.__dot__(flow2).data  # Assuming data attribute exists\n",
    "   ```\n",
    "   - The dot product operation is applied to `flow1` and `flow2` using the `__dot__()` method. The result of the dot product is then accessed via `.data` to retrieve the result stored within the `FlowUnit` object.\n",
    "\n",
    "6. **Using NumPy's Dot Product**:\n",
    "   ```python\n",
    "   np_output = np.dot(data1, data2)\n",
    "   ```\n",
    "   - The `np.dot()` function is used to compute the dot product between `data1` and `data2` in NumPy. The dot product of two vectors \\( \\mathbf{a} = [a_1, a_2, a_3] \\) and \\( \\mathbf{b} = [b_1, b_2, b_3] \\) is computed as:\n",
    "     \\[\n",
    "     \\mathbf{a} \\cdot \\mathbf{b} = a_1 b_1 + a_2 b_2 + a_3 b_3\n",
    "     \\]\n",
    "     For `data1` and `data2`, this results in:\n",
    "     \\[\n",
    "     1 \\times 4 + 2 \\times 5 + 3 \\times 6 = 4 + 10 + 18 = 32\n",
    "     \\]\n",
    "   - The result of this dot product is stored in `np_output`.\n",
    "\n",
    "7. **Comparing the Two Outputs**:\n",
    "   ```python\n",
    "   assert np.isclose(flow_output, np_output), \"Outputs do not match!\"\n",
    "   ```\n",
    "   - The `np.isclose()` function is used to check if `flow_output` and `np_output` are close to each other within a certain tolerance. This function is used because floating-point operations may sometimes result in small numerical differences.\n",
    "   - If the values do not match within the tolerance, an assertion error is raised with the message `\"Outputs do not match!\"`.\n",
    "\n",
    "8. **Printing the Test Result**:\n",
    "   ```python\n",
    "   print(\"Test passed! Both outputs are the same.\")\n",
    "   ```\n",
    "   - If the assertion passes (i.e., the dot products match), the message `\"Test passed! Both outputs are the same.\"` is printed, indicating that the `FlowUnit` dot product operation works as expected.\n",
    "\n",
    "9. **Running the Test**:\n",
    "   ```python\n",
    "   test_flowunit_dot_product()\n",
    "   ```\n",
    "   - The test function `test_flowunit_dot_product()` is called to run the test.\n",
    "\n",
    "### Summary\n",
    "This code defines and runs a test to verify if the dot product operation on two `FlowUnit` instances produces the same result as NumPy's `dot` function. If both results match, a success message is printed; otherwise, an assertion error is raised.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed! Both outputs are the same.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def test_flowunit_dot_product():\n",
    "    # Create sample data (1D vectors)\n",
    "    data1 = np.array([1, 2, 3])\n",
    "    data2 = np.array([4, 5, 6])\n",
    "    \n",
    "    # Creating FlowUnit instances\n",
    "    flow1 = FlowUnit(data1)\n",
    "    flow2 = FlowUnit(data2)\n",
    "    \n",
    "    # Using FlowUnit's dot product\n",
    "    flow_output = flow1.__dot__(flow2).data  # Assuming data attribute exists\n",
    "    \n",
    "    # Using NumPy's dot product\n",
    "    np_output = np.dot(data1, data2)\n",
    "    \n",
    "    # Check if both outputs match\n",
    "    assert np.isclose(flow_output, np_output), \"Outputs do not match!\"\n",
    "    print(\"Test passed! Both outputs are the same.\")\n",
    "\n",
    "# Run the test\n",
    "test_flowunit_dot_product()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "### Code Explanation\n",
    "\n",
    "This Python code defines a test function `test_flowunit_operations()` to verify the correct functionality of various mathematical operations (subtraction, division, power, mean, and logarithm) implemented for the `FlowUnit` class, by comparing them with the corresponding NumPy operations. Here's a detailed step-by-step breakdown of the code:\n",
    "\n",
    "1. **Importing the NumPy Module**:\n",
    "   ```python\n",
    "   import numpy as np\n",
    "   ```\n",
    "   - The `numpy` module is imported as `np`, which is essential for numerical operations such as subtraction, division, power, mean, and logarithm.\n",
    "\n",
    "2. **Defining the Test Function**:\n",
    "   ```python\n",
    "   def test_flowunit_operations():\n",
    "   ```\n",
    "   - The function `test_flowunit_operations()` is defined to test the mathematical operations of the `FlowUnit` class.\n",
    "\n",
    "3. **Creating Sample Data**:\n",
    "   ```python\n",
    "   data1 = np.array([4.0, 9.0, 16.0])\n",
    "   data2 = np.array([2.0, 3.0, 4.0])\n",
    "   ```\n",
    "   - Two 1D NumPy arrays `data1` and `data2` are created to serve as sample data for the operations. These arrays represent vectors of floating-point numbers:\n",
    "     \\[\n",
    "     \\text{data1} = \\begin{bmatrix} 4.0 & 9.0 & 16.0 \\end{bmatrix}, \\quad \\text{data2} = \\begin{bmatrix} 2.0 & 3.0 & 4.0 \\end{bmatrix}\n",
    "     \\]\n",
    "\n",
    "4. **Creating FlowUnit Instances**:\n",
    "   ```python\n",
    "   flow1 = FlowUnit(data1)\n",
    "   flow2 = FlowUnit(data2)\n",
    "   ```\n",
    "   - Two `FlowUnit` instances, `flow1` and `flow2`, are created with `data1` and `data2`, respectively. These instances encapsulate the data and provide methods to perform various operations on the data.\n",
    "\n",
    "5. **Performing Operations Using FlowUnit**:\n",
    "   - The following operations are performed using `FlowUnit` methods:\n",
    "     - **Subtraction**:\n",
    "       ```python\n",
    "       sub_output = flow1.__sub__(flow2).data\n",
    "       ```\n",
    "       - The subtraction operation `flow1 - flow2` is executed using the `__sub__()` method. The result is stored in `sub_output`.\n",
    "     - **Division**:\n",
    "       ```python\n",
    "       div_output = flow1.__truediv__(flow2).data\n",
    "       ```\n",
    "       - The division operation `flow1 / flow2` is executed using the `__truediv__()` method. The result is stored in `div_output`.\n",
    "     - **Power (Exponentiation)**:\n",
    "       ```python\n",
    "       pow_output = flow1.pow(2).data\n",
    "       ```\n",
    "       - The exponentiation operation `flow1 ** 2` is executed using the `pow()` method. The result is stored in `pow_output`.\n",
    "     - **Mean**:\n",
    "       ```python\n",
    "       mean_output = flow1.mean()\n",
    "       ```\n",
    "       - The mean of the values in `flow1` is computed using the `mean()` method. The result is stored in `mean_output`.\n",
    "     - **Logarithm**:\n",
    "       ```python\n",
    "       log_output = flow1.log().data\n",
    "       ```\n",
    "       - The natural logarithm of the values in `flow1` is computed using the `log()` method. The result is stored in `log_output`.\n",
    "\n",
    "6. **Performing Operations Using NumPy**:\n",
    "   - The corresponding NumPy operations are performed as follows:\n",
    "     - **Subtraction**:\n",
    "       ```python\n",
    "       np_sub_output = data1 - data2\n",
    "       ```\n",
    "     - **Division**:\n",
    "       ```python\n",
    "       np_div_output = data1 / data2\n",
    "       ```\n",
    "     - **Power**:\n",
    "       ```python\n",
    "       np_pow_output = np.power(data1, 2)\n",
    "       ```\n",
    "     - **Mean**:\n",
    "       ```python\n",
    "       np_mean_output = np.mean(data1)\n",
    "       ```\n",
    "     - **Logarithm**:\n",
    "       ```python\n",
    "       np_log_output = np.log(data1)\n",
    "       ```\n",
    "\n",
    "7. **Assertions**:\n",
    "   - The test compares the results from `FlowUnit` with those from NumPy to ensure that they match. The `np.allclose()` and `np.isclose()` functions are used to check for equality, allowing for small numerical differences.\n",
    "     - **Subtraction**:\n",
    "       ```python\n",
    "       assert np.allclose(sub_output, np_sub_output), \"Subtraction outputs do not match!\"\n",
    "       ```\n",
    "     - **Division**:\n",
    "       ```python\n",
    "       assert np.allclose(div_output, np_div_output), \"Division outputs do not match!\"\n",
    "       ```\n",
    "     - **Power**:\n",
    "       ```python\n",
    "       assert np.allclose(pow_output, np_pow_output), \"Power outputs do not match!\"\n",
    "       ```\n",
    "     - **Mean**:\n",
    "       ```python\n",
    "       assert np.isclose(mean_output, np_mean_output), \"Mean outputs do not match!\"\n",
    "       ```\n",
    "     - **Logarithm**:\n",
    "       ```python\n",
    "       assert np.allclose(log_output, np_log_output), \"Log outputs do not match!\"\n",
    "       ```\n",
    "\n",
    "8. **Printing the Test Result**:\n",
    "   ```python\n",
    "   print(\"All tests passed! FlowUnit operations match NumPy's results.\")\n",
    "   ```\n",
    "   - If all assertions pass, indicating that the `FlowUnit` operations are correct, the message `\"All tests passed! FlowUnit operations match NumPy's results.\"` is printed.\n",
    "\n",
    "9. **Running the Test**:\n",
    "   ```python\n",
    "   test_flowunit_operations()\n",
    "   ```\n",
    "   - The test function `test_flowunit_operations()` is called to run the test.\n",
    "\n",
    "### Summary\n",
    "This code tests the implementation of several mathematical operations (subtraction, division, power, mean, and logarithm) in the `FlowUnit` class by comparing them with NumPy's corresponding functions. If the results match, a success message is printed, confirming that the operations in `FlowUnit` work correctly.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed! FlowUnit operations match NumPy's results.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def test_flowunit_operations():\n",
    "    # Sample data\n",
    "    data1 = np.array([4.0, 9.0, 16.0])\n",
    "    data2 = np.array([2.0, 3.0, 4.0])\n",
    "    \n",
    "    # Creating FlowUnit instances\n",
    "    flow1 = FlowUnit(data1)\n",
    "    flow2 = FlowUnit(data2)\n",
    "    \n",
    "    # Using FlowUnit operations\n",
    "    sub_output = flow1.__sub__(flow2).data\n",
    "    div_output = flow1.__truediv__(flow2).data\n",
    "    pow_output = flow1.pow(2).data\n",
    "    mean_output = flow1.mean()\n",
    "    log_output = flow1.log().data\n",
    "    \n",
    "    # Using NumPy equivalent operations\n",
    "    np_sub_output = data1 - data2\n",
    "    np_div_output = data1 / data2\n",
    "    np_pow_output = np.power(data1, 2)\n",
    "    np_mean_output = np.mean(data1)\n",
    "    np_log_output = np.log(data1)\n",
    "\n",
    "    # Assertions\n",
    "    assert np.allclose(sub_output, np_sub_output), \"Subtraction outputs do not match!\"\n",
    "    assert np.allclose(div_output, np_div_output), \"Division outputs do not match!\"\n",
    "    assert np.allclose(pow_output, np_pow_output), \"Power outputs do not match!\"\n",
    "    assert np.isclose(mean_output, np_mean_output), \"Mean outputs do not match!\"\n",
    "    assert np.allclose(log_output, np_log_output), \"Log outputs do not match!\"\n",
    "\n",
    "    print(\"All tests passed! FlowUnit operations match NumPy's results.\")\n",
    "\n",
    "# Run the test\n",
    "test_flowunit_operations()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Code Explanation\n",
    "\n",
    "1. **Importing NumPy**:\n",
    "   ```python\n",
    "   import numpy as np\n",
    "   ```\n",
    "   - `numpy` is imported as `np` to perform vectorized operations and mathematical functions.\n",
    "\n",
    "2. **Defining the Test Function**:\n",
    "   ```python\n",
    "   def test_flowunit_arrays():\n",
    "   ```\n",
    "   - The `test_flowunit_arrays()` function is defined to test several operations (subtraction, division, exponentiation, activations, and dot product) with arrays.\n",
    "\n",
    "3. **Test Arrays Creation**:\n",
    "   ```python\n",
    "   np_a = np.array([1.0, 2.0, 3.0])\n",
    "   np_b = np.array([4.0, 5.0, 6.0])\n",
    "   ```\n",
    "   - Two 1D NumPy arrays `np_a` and `np_b` are created to represent vectors for testing.\n",
    "\n",
    "4. **FlowUnit Instances**:\n",
    "   ```python\n",
    "   a = FlowUnit(np_a)\n",
    "   b = FlowUnit(np_b)\n",
    "   ```\n",
    "   - `FlowUnit` instances `a` and `b` are created with `np_a` and `np_b` as input data.\n",
    "\n",
    "5. **Array Operations**:\n",
    "   - **Subtraction**:\n",
    "     ```python\n",
    "     assert np.allclose((a - b).data, np_a - np_b)\n",
    "     ```\n",
    "     - The subtraction operation between `a` and `b` is tested and compared with the result of `np_a - np_b`.\n",
    "   - **Division**:\n",
    "     ```python\n",
    "     assert np.allclose((a / b).data, np_a / np_b)\n",
    "     ```\n",
    "     - The division operation between `a` and `b` is tested and compared with `np_a / np_b`.\n",
    "   - **Exponentiation**:\n",
    "     ```python\n",
    "     assert np.allclose(a.pow(2).data, np_a ** 2)\n",
    "     ```\n",
    "     - The exponentiation operation `a.pow(2)` is compared with `np_a ** 2`.\n",
    "\n",
    "6. **Activation Functions**:\n",
    "   - **Sigmoid**:\n",
    "     ```python\n",
    "     assert np.allclose(a.sigmoid().data, 1 / (1 + np.exp(-np_a)))\n",
    "     ```\n",
    "     - The sigmoid activation is tested by comparing `a.sigmoid()` with the NumPy version.\n",
    "   - **Tanh**:\n",
    "     ```python\n",
    "     assert np.allclose(a.tanh().data, np.tanh(np_a))\n",
    "     ```\n",
    "     - The tanh activation is tested using `a.tanh()` and `np.tanh(np_a)`.\n",
    "   - **ReLU**:\n",
    "     ```python\n",
    "     assert np.allclose(a.relu().data, np.maximum(0, np_a))\n",
    "     ```\n",
    "     - The ReLU activation is tested with `a.relu()` and `np.maximum(0, np_a)`.\n",
    "   - **Leaky ReLU**:\n",
    "     ```python\n",
    "     assert np.allclose(a.leaky_relu().data, np.where(np_a > 0, np_a, 0.01 * np_a))\n",
    "     ```\n",
    "     - The Leaky ReLU activation is tested by comparing `a.leaky_relu()` with `np.where(np_a > 0, np_a, 0.01 * np_a)`.\n",
    "\n",
    "7. **Dot Product**:\n",
    "   ```python\n",
    "   assert np.allclose(a.__dot__(b).data, np.dot(np_a, np_b))\n",
    "   ```\n",
    "   - The dot product operation is tested by comparing `a.__dot__(b)` with `np.dot(np_a, np_b)`.\n",
    "\n",
    "8. **Printing Success**:\n",
    "   ```python\n",
    "   print(\"All array tests passed!\")\n",
    "   ```\n",
    "   - If all assertions pass, a success message `\"All array tests passed!\"` is printed.\n",
    "\n",
    "9. **Running the Tests**:\n",
    "   ```python\n",
    "   test_flowunit_arrays()\n",
    "   ```\n",
    "   - Finally, the `test_flowunit_arrays()` function is called to execute the tests.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All array tests passed!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def test_flowunit_arrays():\n",
    "    # Test with arrays\n",
    "   \n",
    "\n",
    "    np_a = np.array([1.0, 2.0, 3.0])\n",
    "    np_b = np.array([4.0, 5.0, 6.0])\n",
    "    \n",
    "    a = FlowUnit(np_a)\n",
    "    b = FlowUnit(np_b)\n",
    "\n",
    "    assert np.allclose((a - b).data, np_a - np_b)\n",
    "    assert np.allclose((a / b).data, np_a / np_b)\n",
    "    assert np.allclose(a.pow(2).data, np_a ** 2)\n",
    "    \n",
    "    # Test activations\n",
    "    assert np.allclose(a.sigmoid().data, 1 / (1 + np.exp(-np_a)))\n",
    "    assert np.allclose(a.tanh().data, np.tanh(np_a))\n",
    "    assert np.allclose(a.relu().data, np.maximum(0, np_a))\n",
    "    assert np.allclose(a.leaky_relu().data, np.where(np_a > 0, np_a, 0.01 * np_a))\n",
    "    \n",
    "    # Test dot product\n",
    "    assert np.allclose(a.__dot__(b).data, np.dot(np_a, np_b))\n",
    "\n",
    "    print(\"All array tests passed!\")\n",
    "\n",
    "# Run tests\n",
    "test_flowunit_arrays()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an explanation of what your code is doing:\n",
    "\n",
    "1. **Test Data Setup**:  \n",
    "   ```python\n",
    "   test_values = [[2.0, 1.0, 0.1], [1.0, 2.0, 3.0], [-1.0, 0.0, 1.0]]\n",
    "   ```\n",
    "   - A list of different test input arrays (sets of values) to test the softmax function.\n",
    "\n",
    "2. **Loop Through Test Data**:  \n",
    "   ```python\n",
    "   for vals in test_values:\n",
    "   ```\n",
    "   - The code loops through each set of test values in `test_values`.\n",
    "\n",
    "3. **FlowUnit Softmax**:  \n",
    "   ```python\n",
    "   x = FlowUnit(vals)\n",
    "   softmax_out = x.softmax()\n",
    "   ```\n",
    "   - A `FlowUnit` object `x` is created using the test values `vals`.\n",
    "   - The `softmax()` method is called on the `FlowUnit` instance, which computes the softmax of the input values and stores the result in `softmax_out`.\n",
    "\n",
    "4. **NumPy Softmax Calculation**:  \n",
    "   ```python\n",
    "   np_softmax = np.exp(vals - np.max(vals)) / np.sum(np.exp(vals - np.max(vals)))\n",
    "   ```\n",
    "   - NumPy is used to calculate the softmax for the same values using the formula:\n",
    "     \\[\n",
    "     \\text{softmax}(x_i) = \\frac{e^{x_i - \\max(x)}}{\\sum_j e^{x_j - \\max(x)}}\n",
    "     \\]\n",
    "     This formula ensures numerical stability by subtracting the maximum value from the inputs.\n",
    "\n",
    "5. **Compare Softmax Results**:  \n",
    "   ```python\n",
    "   assert np.allclose(softmax_out.data, np_softmax, atol=1e-6), \\\n",
    "       f\"Mismatch: {softmax_out.data} != {np_softmax}\"\n",
    "   ```\n",
    "   - The results from the `FlowUnit` softmax (`softmax_out.data`) are compared to the NumPy softmax values (`np_softmax`).\n",
    "   - The comparison is done using `np.allclose`, which checks if the two results are approximately equal within a tolerance of `1e-6`.\n",
    "\n",
    "6. **Success Message**:  \n",
    "   ```python\n",
    "   print(\"All softmax tests passed successfully!\")\n",
    "   ```\n",
    "   - If all the tests pass, a success message is printed to indicate that the softmax function in `FlowUnit` matches NumPy's implementation.\n",
    "\n",
    "In summary, the code tests the softmax function in the `FlowUnit` class by comparing its output against NumPy's softmax calculation, ensuring they match within a small tolerance for various test inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All softmax tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def test_softmax():\n",
    "    \"\"\"Test the softmax function in FlowUnit against numpy's implementation.\"\"\"\n",
    "    test_values = [[2.0, 1.0, 0.1], [1.0, 2.0, 3.0], [-1.0, 0.0, 1.0]]\n",
    "\n",
    "    for vals in test_values:\n",
    "        # FlowUnit softmax\n",
    "        x = FlowUnit(vals)\n",
    "        softmax_out = x.softmax()\n",
    "\n",
    "        # Numpy softmax for comparison\n",
    "        np_softmax = np.exp(vals - np.max(vals)) / np.sum(np.exp(vals - np.max(vals)))\n",
    "\n",
    "        # Check if values match (within a tolerance)\n",
    "        assert np.allclose(softmax_out.data, np_softmax, atol=1e-6), \\\n",
    "            f\"Mismatch: {softmax_out.data} != {np_softmax}\"\n",
    "\n",
    "    print(\"All softmax tests passed successfully!\")\n",
    "\n",
    "# Run the test\n",
    "test_softmax()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code you have written aims to test backpropagation through a series of operations in a neural network-like scenario, using the `FlowUnit` class. Here's an explanation of each step in the test:\n",
    "\n",
    "1. **Create FlowUnit Instances**:\n",
    "   ```python\n",
    "   x = FlowUnit(2.0)  # Input value\n",
    "   y = FlowUnit(-3.0)\n",
    "   z = FlowUnit(1.5)\n",
    "   ```\n",
    "   - Three `FlowUnit` instances are created with scalar values: `x = 2.0`, `y = -3.0`, and `z = 1.5`.\n",
    "\n",
    "2. **Forward Computations**:\n",
    "   ```python\n",
    "   a = x.sigmoid()\n",
    "   b = y.tanh()\n",
    "   c = z.relu()\n",
    "   d = x.leaky_relu()\n",
    "   ```\n",
    "   - The code performs a series of operations on the input values:\n",
    "     - `a` is the result of applying the sigmoid activation function to `x`.\n",
    "     - `b` is the result of applying the tanh activation function to `y`.\n",
    "     - `c` is the result of applying the ReLU activation function to `z`.\n",
    "     - `d` is the result of applying the leaky ReLU activation function to `x`.\n",
    "\n",
    "3. **Dummy Loss Function**:\n",
    "   ```python\n",
    "   loss = a + b + c + d\n",
    "   ```\n",
    "   - A simple loss function is defined by summing the outputs of all the activation functions (`a`, `b`, `c`, `d`). This acts as the loss that will be used to compute gradients during backpropagation.\n",
    "\n",
    "4. **Backpropagation**:\n",
    "   ```python\n",
    "   loss.backpropagate()\n",
    "   ```\n",
    "   - The `backpropagate()` method is called on the loss to compute the gradients of all the inputs with respect to this loss. This simulates how a neural network adjusts its weights based on the computed gradients.\n",
    "\n",
    "5. **Print Gradients**:\n",
    "   ```python\n",
    "   print(f\"x.grad: {x.grad}\")\n",
    "   print(f\"y.grad: {y.grad}\")\n",
    "   print(f\"z.grad: {z.grad}\")\n",
    "   print(f\"a.grad (sigmoid): {a.grad}\")\n",
    "   print(f\"b.grad (tanh): {b.grad}\")\n",
    "   print(f\"c.grad (ReLU): {c.grad}\")\n",
    "   print(f\"d.grad (Leaky ReLU): {d.grad}\")\n",
    "   ```\n",
    "   - The gradients of the variables and intermediate results are printed. This includes the gradients with respect to the input variables (`x`, `y`, `z`) and the gradients with respect to the outputs of each activation function (`a`, `b`, `c`, `d`).\n",
    "\n",
    "In summary, the code is testing how backpropagation works through multiple types of operations (sigmoid, tanh, ReLU, and leaky ReLU). It calculates the gradients of the inputs and outputs during the backpropagation step and prints them. For this to work, the `FlowUnit` class must support these operations and backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad: 1.1049935854035067\n",
      "y.grad: 0.00986603716543999\n",
      "z.grad: 1.0\n",
      "a.grad (sigmoid): 1.0\n",
      "b.grad (tanh): 1.0\n",
      "c.grad (ReLU): 1.0\n",
      "d.grad (Leaky ReLU): 1.0\n"
     ]
    }
   ],
   "source": [
    "def test_backpropagation():\n",
    "    \"\"\"Test backpropagation through multiple FlowUnit operations.\"\"\"\n",
    "    x = FlowUnit(2.0)  # Input value\n",
    "    y = FlowUnit(-3.0)\n",
    "    z = FlowUnit(1.5)\n",
    "\n",
    "    # Forward computations\n",
    "    a = x.sigmoid()\n",
    "    b = y.tanh()\n",
    "    c = z.relu()\n",
    "    d = x.leaky_relu()\n",
    "\n",
    "       \n",
    "\n",
    "    # Dummy loss function: Sum of all outputs\n",
    "    loss = a + b + c + d \n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backpropagate()\n",
    "\n",
    "    # Print gradients\n",
    "    print(f\"x.grad: {x.grad}\")\n",
    "    print(f\"y.grad: {y.grad}\")\n",
    "    print(f\"z.grad: {z.grad}\")\n",
    "    print(f\"a.grad (sigmoid): {a.grad}\")\n",
    "    print(f\"b.grad (tanh): {b.grad}\")\n",
    "    print(f\"c.grad (ReLU): {c.grad}\")\n",
    "    print(f\"d.grad (Leaky ReLU): {d.grad}\")\n",
    "  \n",
    "\n",
    "# Run the test function\n",
    "test_backpropagation()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code you've provided tests backpropagation through multiple operations in PyTorch, similar to the previous `FlowUnit` example. Here's an explanation of how this code works:\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "1. **Define Input Tensors with `requires_grad=True`**:\n",
    "   ```python\n",
    "   x = torch.tensor(2.0, requires_grad=True)\n",
    "   y = torch.tensor(-3.0, requires_grad=True)\n",
    "   z = torch.tensor(1.5, requires_grad=True)\n",
    "   ```\n",
    "   - The variables `x`, `y`, and `z` are created as PyTorch tensors, with `requires_grad=True`, indicating that we want to compute the gradients with respect to these variables during backpropagation.\n",
    "\n",
    "2. **Forward Computations Using PyTorch Functions**:\n",
    "   ```python\n",
    "   a = torch.sigmoid(x)\n",
    "   b = torch.tanh(y)\n",
    "   c = torch.relu(z)\n",
    "   d = F.leaky_relu(x, negative_slope=0.01)\n",
    "   ```\n",
    "   - `a` is the result of applying the sigmoid function to `x`.\n",
    "   - `b` is the result of applying the tanh function to `y`.\n",
    "   - `c` is the result of applying the ReLU activation to `z`.\n",
    "   - `d` is the result of applying the leaky ReLU function to `x` with a negative slope of 0.01.\n",
    "\n",
    "3. **Retaining Gradients for Intermediate Results**:\n",
    "   ```python\n",
    "   a.retain_grad()\n",
    "   b.retain_grad()\n",
    "   c.retain_grad()\n",
    "   d.retain_grad()\n",
    "   ```\n",
    "   - The `retain_grad()` function is used to store the gradients for intermediate results (`a`, `b`, `c`, `d`). This is needed to access the gradients after backpropagation.\n",
    "\n",
    "4. **Dummy Loss Function**:\n",
    "   ```python\n",
    "   loss = a + b + c + d\n",
    "   ```\n",
    "   - A simple loss function is defined by summing all the intermediate results. This will be used to compute the gradients.\n",
    "\n",
    "5. **Backpropagation**:\n",
    "   ```python\n",
    "   loss.backward()\n",
    "   ```\n",
    "   - The `backward()` method computes the gradients of the `loss` with respect to the tensors `x`, `y`, and `z`. It propagates the error backward through the computation graph.\n",
    "\n",
    "6. **Print Gradients**:\n",
    "   ```python\n",
    "   print(f\"PyTorch - x.grad: {x.grad}\")\n",
    "   print(f\"PyTorch - y.grad: {y.grad}\")\n",
    "   print(f\"PyTorch - z.grad: {z.grad}\")\n",
    "   print(f\"PyTorch - a.grad (sigmoid): {a.grad}\")\n",
    "   print(f\"PyTorch - b.grad (tanh): {b.grad}\")\n",
    "   print(f\"PyTorch - c.grad (ReLU): {c.grad}\")\n",
    "   print(f\"PyTorch - d.grad (Leaky ReLU): {d.grad}\")\n",
    "   ```\n",
    "   - After backpropagation, the gradients for the input tensors (`x`, `y`, `z`) and the intermediate results (`a`, `b`, `c`, `d`) are printed. These gradients are computed using the chain rule in the backward pass.\n",
    "\n",
    "### Expected Output:\n",
    "- The gradients of each tensor (`x`, `y`, `z`) and each intermediate result (`a`, `b`, `c`, `d`) will be printed.\n",
    "- PyTorch calculates the gradients automatically based on the operations performed, using the chain rule during backpropagation.\n",
    "\n",
    "This test verifies that backpropagation works correctly through multiple common operations in PyTorch, such as sigmoid, tanh, ReLU, and leaky ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch - x.grad: 1.1049935817718506\n",
      "PyTorch - y.grad: 0.009865999221801758\n",
      "PyTorch - z.grad: 1.0\n",
      "PyTorch - a.grad (sigmoid): 1.0\n",
      "PyTorch - b.grad (tanh): 1.0\n",
      "PyTorch - c.grad (ReLU): 1.0\n",
      "PyTorch - d.grad (Leaky ReLU): 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "def test_backpropagation_pytorch():\n",
    "    \"\"\"Test backpropagation through multiple PyTorch operations.\"\"\"\n",
    "    # Define input tensors with requires_grad=True to track gradients\n",
    "    x = torch.tensor(2.0, requires_grad=True)  # Input value for x\n",
    "    y = torch.tensor(-3.0, requires_grad=True)  # Input value for y\n",
    "    z = torch.tensor(1.5, requires_grad=True)  # Input value for z\n",
    "\n",
    "    # Forward computations using PyTorch operations\n",
    "    a = torch.sigmoid(x)\n",
    "    b = torch.tanh(y)\n",
    "    c = torch.relu(z)\n",
    "    d = F.leaky_relu(x, negative_slope=0.01)\n",
    "\n",
    "    # Retain gradients for intermediate results\n",
    "    a.retain_grad()\n",
    "    b.retain_grad()\n",
    "    c.retain_grad()\n",
    "    d.retain_grad()\n",
    "\n",
    "    # Dummy loss function: Sum of all outputs\n",
    "    loss = a + b + c + d\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # Print gradients\n",
    "    print(f\"PyTorch - x.grad: {x.grad}\")\n",
    "    print(f\"PyTorch - y.grad: {y.grad}\")\n",
    "    print(f\"PyTorch - z.grad: {z.grad}\")\n",
    "    print(f\"PyTorch - a.grad (sigmoid): {a.grad}\")\n",
    "    print(f\"PyTorch - b.grad (tanh): {b.grad}\")\n",
    "    print(f\"PyTorch - c.grad (ReLU): {c.grad}\")\n",
    "    print(f\"PyTorch - d.grad (Leaky ReLU): {d.grad}\")\n",
    "\n",
    "# Run the test function\n",
    "test_backpropagation_pytorch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code you've written tests the softmax function followed by the computation of the categorical cross-entropy loss and backpropagation in a custom `FlowUnit` implementation. Here's a breakdown of what the code is doing:\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "1. **Create Input Logits**:\n",
    "   ```python\n",
    "   logits = FlowUnit([7.0, 1.0, 0.5])\n",
    "   ```\n",
    "   - `logits` is a `FlowUnit` instance that holds the raw output (unnormalized predictions) for three classes. These values will be input to the softmax function to get the probabilities for each class.\n",
    "\n",
    "2. **Create Target (One-Hot Encoded)**:\n",
    "   ```python\n",
    "   target = [1, 0, 0]  # First class is the true class\n",
    "   ```\n",
    "   - `target` is the true label in one-hot encoded form. It indicates that the true class is the first one (class index 0).\n",
    "\n",
    "3. **Compute Loss Using Categorical Cross-Entropy**:\n",
    "   ```python\n",
    "   loss = LossFunctions.categorical_cross_entropy(logits, target)\n",
    "   ```\n",
    "   - The loss is computed using the `categorical_cross_entropy` function from the `LossFunctions` module. This function takes the logits and the target labels to compute the loss. Categorical cross-entropy compares the predicted probability distribution (from softmax) with the true class probabilities (one-hot encoded target).\n",
    "\n",
    "4. **Backpropagate the Loss**:\n",
    "   ```python\n",
    "   loss.backpropagate()\n",
    "   ```\n",
    "   - After the loss is computed, `backpropagate()` is called to propagate the gradients back through the computation graph. This updates the gradients for the input (`logits`), based on the loss.\n",
    "\n",
    "5. **Print Results**:\n",
    "   ```python\n",
    "   print(\"Softmax probabilities:\", logits.softmax().data)\n",
    "   print(\"Gradients after loss:\", logits.grad)\n",
    "   print(\"Loss value:\", loss.data)\n",
    "   ```\n",
    "   - `logits.softmax().data` computes the softmax of the logits, which normalizes them into probabilities (i.e., the predicted probability for each class).\n",
    "   - `logits.grad` prints the gradients of the input logits after backpropagation.\n",
    "   - `loss.data` prints the computed loss value.\n",
    "\n",
    "### Expected Output:\n",
    "- **Softmax probabilities**: The result of applying softmax to the logits, representing the predicted probability distribution for the classes.\n",
    "- **Gradients after loss**: The gradients for the logits, indicating how much each logit needs to change to reduce the loss.\n",
    "- **Loss value**: The computed categorical cross-entropy loss based on the predicted probabilities and the true class label.\n",
    "\n",
    "### Summary:\n",
    "This test verifies that the `FlowUnit` class can correctly handle the softmax function, compute the categorical cross-entropy loss, and perform backpropagation to update the gradients. It also ensures that the gradients and loss values are correctly computed and accessible after the loss function and backpropagation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax probabilities: [0.9960336035799486, 0.0024689204629066348, 0.0014974759571447812]\n",
      "Gradients after loss: 0.0\n",
      "Loss value: 0.003974273392763471\n"
     ]
    }
   ],
   "source": [
    "from FlowUnit_module import LossFunctions\n",
    "def test_softmax_with_loss():\n",
    "    # Create input logits\n",
    "    logits = FlowUnit([7.0, 1.0, 0.5])\n",
    "    \n",
    "    \n",
    "    # Create target (one-hot encoded)\n",
    "    target = [1, 0, 0]  # First class is the true class\n",
    "    \n",
    "    # Compute loss using categorical cross entropy\n",
    "    loss = LossFunctions.categorical_cross_entropy(logits, target)\n",
    "    \n",
    "    # Backpropagate\n",
    "    loss.backpropagate()\n",
    "    \n",
    "    # Now logits.grad should show non-zero values\n",
    "    print(\"Softmax probabilities:\", logits.softmax().data)\n",
    "    print(\"Gradients after loss:\", logits.grad)\n",
    "    print(\"Loss value:\", loss.data)\n",
    "    \n",
    "\n",
    "test_softmax_with_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code sets up a basic unit test for the `FlowUnit` class using Python's built-in `unittest` framework. Here's an explanation of what each part does:\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "1. **Test Class Declaration**:\n",
    "   ```python\n",
    "   class TestFlowUnit(unittest.TestCase):\n",
    "   ```\n",
    "   - The `TestFlowUnit` class inherits from `unittest.TestCase`, which allows us to use various methods provided by `unittest` to write and run tests.\n",
    "\n",
    "2. **Test Method**:\n",
    "   ```python\n",
    "   def test_addition(self):\n",
    "   ```\n",
    "   - The method `test_addition` will test the addition operation of two `FlowUnit` instances. The method name should start with `test_` so that `unittest` can identify it as a test case.\n",
    "\n",
    "3. **Creating FlowUnit Instances**:\n",
    "   ```python\n",
    "   x = FlowUnit(2)\n",
    "   y = FlowUnit(3)\n",
    "   ```\n",
    "   - Two `FlowUnit` objects (`x` and `y`) are created with data values 2 and 3, respectively.\n",
    "\n",
    "4. **Addition of FlowUnit Instances**:\n",
    "   ```python\n",
    "   z = x + y\n",
    "   ```\n",
    "   - The addition operator `+` is used to add `x` and `y`. This will trigger the `__add__` method in the `FlowUnit` class, resulting in a new `FlowUnit` object (`z`).\n",
    "\n",
    "5. **Setting Gradient for Backpropagation**:\n",
    "   ```python\n",
    "   z.grad = 1.0\n",
    "   z.backward()\n",
    "   ```\n",
    "   - The gradient of `z` is set to 1.0, and the `backward()` method is called. This triggers backpropagation to compute the gradients for `x` and `y`.\n",
    "\n",
    "6. **Printing Data and Gradients**:\n",
    "   ```python\n",
    "   print(f\"x.data = {x.data}, y.data = {y.data}, z.data = {z.data}\")\n",
    "   print(f\"x.grad = {x.grad}, y.grad = {y.grad}\")\n",
    "   ```\n",
    "   - The `.data` and `.grad` attributes of `x`, `y`, and `z` are printed. These show the values and the gradients that have been computed after the backward pass.\n",
    "\n",
    "7. **Assertions**:\n",
    "   ```python\n",
    "   self.assertEqual(x.grad, 1.0, \"Gradient for x is incorrect\")\n",
    "   self.assertEqual(y.grad, 1.0, \"Gradient for y is incorrect\")\n",
    "   ```\n",
    "   - The `assertEqual` method is used to check if the gradients of `x` and `y` are equal to 1.0, as expected. If the gradients are incorrect, an error message will be displayed.\n",
    "\n",
    "8. **Running the Tests**:\n",
    "   ```python\n",
    "   unittest.TextTestRunner().run(unittest.TestLoader().loadTestsFromTestCase(TestFlowUnit))\n",
    "   ```\n",
    "   - This line loads the test case from the `TestFlowUnit` class and runs it using the `unittest.TextTestRunner()`. This will execute the `test_addition` method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.002s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.data = 2, y.data = 3, z.data = 5\n",
      "x.grad = 1.0, y.grad = 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestFlowUnit(unittest.TestCase):\n",
    "    # Your test methods here\n",
    "    def test_addition(self):\n",
    "        x = FlowUnit(2)\n",
    "        y = FlowUnit(3)\n",
    "        z = x + y\n",
    "        z.grad = 1.0\n",
    "        z.backward()\n",
    "        print(f\"x.data = {x.data}, y.data = {y.data}, z.data = {z.data}\")\n",
    "        print(f\"x.grad = {x.grad}, y.grad = {y.grad}\")\n",
    "        self.assertEqual(x.grad, 1.0, \"Gradient for x is incorrect\")\n",
    "        self.assertEqual(y.grad, 1.0, \"Gradient for y is incorrect\")\n",
    "\n",
    "# Run unittest in the notebook\n",
    "unittest.TextTestRunner().run(unittest.TestLoader().loadTestsFromTestCase(TestFlowUnit))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines two test functions: `test_flow_unit_functions()` and `test_loss_functions()`, which are used to test various operations in the `FlowUnit` class and the loss functions from the `LossFunctions` class. Here's an explanation of what each part does:\n",
    "\n",
    "### 1. **Test FlowUnit Class Functions (`test_flow_unit_functions`)**:\n",
    "   This function tests the common activation functions in the `FlowUnit` class and verifies backpropagation on each operation.\n",
    "\n",
    "   - **Create FlowUnit instance**:\n",
    "     ```python\n",
    "     data = np.array([1.0, 2.0, 3.0])\n",
    "     flow_unit = FlowUnit(data)\n",
    "     ```\n",
    "     - A `FlowUnit` instance is created using a numpy array `[1.0, 2.0, 3.0]` as the data.\n",
    "\n",
    "   - **Test Sigmoid Function**:\n",
    "     ```python\n",
    "     sigmoid_out = flow_unit.sigmoid()\n",
    "     print(\"Sigmoid Output:\", sigmoid_out.data)\n",
    "     sigmoid_out.backpropagate()\n",
    "     print(\"Sigmoid Gradient:\", flow_unit.grad)\n",
    "     ```\n",
    "     - The `sigmoid()` method is applied to `flow_unit`. It returns a new `FlowUnit` with the result of applying the sigmoid activation. Afterward, `backpropagate()` is called to compute gradients.\n",
    "\n",
    "   - **Test Tanh Function**:\n",
    "     ```python\n",
    "     tanh_out = flow_unit.tanh()\n",
    "     print(\"Tanh Output:\", tanh_out.data)\n",
    "     tanh_out.backpropagate()\n",
    "     print(\"Tanh Gradient:\", flow_unit.grad)\n",
    "     ```\n",
    "     - Similarly, the `tanh()` function is applied to `flow_unit`, followed by backpropagation to compute gradients.\n",
    "\n",
    "   - **Test ReLU Function**:\n",
    "     ```python\n",
    "     relu_out = flow_unit.relu()\n",
    "     print(\"ReLU Output:\", relu_out.data)\n",
    "     relu_out.backpropagate()\n",
    "     print(\"ReLU Gradient:\", flow_unit.grad)\n",
    "     ```\n",
    "     - The `relu()` function is tested, with output and gradient printed.\n",
    "\n",
    "   - **Test Leaky ReLU Function**:\n",
    "     ```python\n",
    "     leaky_relu_out = flow_unit.leaky_relu(alpha=0.01)\n",
    "     print(\"Leaky ReLU Output:\", leaky_relu_out.data)\n",
    "     leaky_relu_out.backpropagate()\n",
    "     print(\"Leaky ReLU Gradient:\", flow_unit.grad)\n",
    "     ```\n",
    "     - The `leaky_relu()` method is tested, with the output and gradient printed. Here, the `alpha` parameter is set to `0.01`.\n",
    "\n",
    "   - **Test Softmax Function**:\n",
    "     ```python\n",
    "     softmax_out = flow_unit.softmax()\n",
    "     print(\"Softmax Output:\", softmax_out.data)\n",
    "     softmax_out.backpropagate()\n",
    "     print(\"Softmax Gradient:\", flow_unit.grad)\n",
    "     ```\n",
    "     - The `softmax()` function is tested, followed by backpropagation to print the computed gradient.\n",
    "\n",
    "### 2. **Test LossFunctions Class Functions (`test_loss_functions`)**:\n",
    "   This function tests the loss functions from the `LossFunctions` class, including categorical cross-entropy, binary cross-entropy, and mean squared error.\n",
    "\n",
    "   - **Test Categorical Cross-Entropy**:\n",
    "     ```python\n",
    "     logits = FlowUnit(np.array([2.0, 1.0, 0.1]))\n",
    "     target = [1, 0, 0]\n",
    "     cce_loss = LossFunctions.categorical_cross_entropy(logits, target)\n",
    "     print(\"Categorical Cross-Entropy Loss:\", cce_loss.data)\n",
    "     cce_loss.backpropagate()\n",
    "     print(\"Categorical Cross-Entropy Gradient:\", logits.grad)\n",
    "     ```\n",
    "     - The `categorical_cross_entropy()` method is tested on the `logits` and `target`. The computed loss is printed, followed by backpropagation to print the gradient.\n",
    "\n",
    "   - **Test Binary Cross-Entropy**:\n",
    "     ```python\n",
    "     inputs = FlowUnit(np.array([[1.0, 2.0], [3.0, 4.0]]))\n",
    "     target = FlowUnit(np.array([1, 0]))\n",
    "     parameters = (FlowUnit(np.array([0.5, -0.5])), FlowUnit(0.1))\n",
    "     bce_loss = LossFunctions.binary_cross_entropy_loss(inputs, target, parameters)\n",
    "     print(\"Binary Cross-Entropy Loss:\", bce_loss.data)\n",
    "     bce_loss.backpropagate()\n",
    "     print(\"Binary Cross-Entropy Gradient:\", inputs.grad)\n",
    "     ```\n",
    "     - The `binary_cross_entropy_loss()` function is tested, using input and target tensors, as well as parameters. The loss value and gradient are printed.\n",
    "\n",
    "   - **Test Mean Squared Error Loss**:\n",
    "     ```python\n",
    "     mse_loss = LossFunctions.mse_loss(inputs, target, parameters)\n",
    "     print(\"Mean Squared Error Loss:\", mse_loss.data)\n",
    "     mse_loss.backpropagate()\n",
    "     print(\"Mean Squared Error Gradient:\", inputs.grad)\n",
    "     ```\n",
    "     - The `mse_loss()` function is tested on `inputs`, `target`, and `parameters`. The loss and gradient are printed after backpropagation.\n",
    "\n",
    "### Expected Output:\n",
    "- **Sigmoid, Tanh, ReLU, Leaky ReLU, Softmax**: For each of these activation functions, the output values will be printed, followed by the gradients calculated after calling `backpropagate()`.\n",
    "  \n",
    "- **Loss Functions**: For each loss function (Categorical Cross-Entropy, Binary Cross-Entropy, MSE), the loss value will be printed along with the gradients of the inputs or logits after backpropagation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid Output: [0.73105858 0.88079708 0.95257413]\n",
      "Sigmoid Gradient: [0.19661193 0.10499359 0.04517666]\n",
      "Tanh Output: [0.76159416 0.96402758 0.99505475]\n",
      "Tanh Gradient: [0.61658627 0.17564441 0.0550427 ]\n",
      "ReLU Output: [1. 2. 3.]\n",
      "ReLU Gradient: [1.61658627 1.17564441 1.0550427 ]\n",
      "Leaky ReLU Output: [1. 2. 3.]\n",
      "Leaky ReLU Gradient: [2.61658627 2.17564441 2.0550427 ]\n",
      "Softmax Output: [0.09003057317038046, 0.24472847105479764, 0.6652409557748218]\n",
      "Softmax Gradient: [1.3877787807814457e-17, 2.7755575615628914e-17, 8.326672684688674e-17]\n",
      "Categorical Cross-Entropy Loss: 0.4170300011033529\n",
      "Categorical Cross-Entropy Gradient: 0.0\n",
      "Binary Cross-Entropy Loss: [0.71561525]\n",
      "Binary Cross-Entropy Gradient: 0.0\n",
      "Mean Squared Error Loss: [0.655]\n",
      "Mean Squared Error Gradient: 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test the FlowUnit class functions\n",
    "def test_flow_unit_functions():\n",
    "    # Create a FlowUnit instance\n",
    "    data = np.array([1.0, 2.0, 3.0])\n",
    "    flow_unit = FlowUnit(data)\n",
    "\n",
    "    # Test sigmoid\n",
    "    sigmoid_out = flow_unit.sigmoid()\n",
    "    print(\"Sigmoid Output:\", sigmoid_out.data)\n",
    "    sigmoid_out.backpropagate()\n",
    "    print(\"Sigmoid Gradient:\", flow_unit.grad)\n",
    "\n",
    "    # Test tanh\n",
    "    tanh_out = flow_unit.tanh()\n",
    "    print(\"Tanh Output:\", tanh_out.data)\n",
    "    tanh_out.backpropagate()\n",
    "    print(\"Tanh Gradient:\", flow_unit.grad)\n",
    "\n",
    "    # Test relu\n",
    "    relu_out = flow_unit.relu()\n",
    "    print(\"ReLU Output:\", relu_out.data)\n",
    "    relu_out.backpropagate()\n",
    "    print(\"ReLU Gradient:\", flow_unit.grad)\n",
    "\n",
    "    # Test leaky_relu\n",
    "    leaky_relu_out = flow_unit.leaky_relu(alpha=0.01)\n",
    "    print(\"Leaky ReLU Output:\", leaky_relu_out.data)\n",
    "    leaky_relu_out.backpropagate()\n",
    "    print(\"Leaky ReLU Gradient:\", flow_unit.grad)\n",
    "\n",
    "    # Test softmax\n",
    "    softmax_out = flow_unit.softmax()\n",
    "    print(\"Softmax Output:\", softmax_out.data)\n",
    "    softmax_out.backpropagate()\n",
    "    print(\"Softmax Gradient:\", flow_unit.grad)\n",
    "\n",
    "# Test the LossFunctions class functions\n",
    "def test_loss_functions():\n",
    "    # Create FlowUnit instances for logits and target\n",
    "    logits = FlowUnit(np.array([2.0, 1.0, 0.1]))\n",
    "    target = [1, 0, 0]  # One-hot encoded target\n",
    "\n",
    "    # Test categorical_cross_entropy\n",
    "    cce_loss = LossFunctions.categorical_cross_entropy(logits, target)\n",
    "    print(\"Categorical Cross-Entropy Loss:\", cce_loss.data)\n",
    "    cce_loss.backpropagate()\n",
    "    print(\"Categorical Cross-Entropy Gradient:\", logits.grad)\n",
    "\n",
    "    # Create FlowUnit instances for inputs and target\n",
    "    inputs = FlowUnit(np.array([[1.0, 2.0], [3.0, 4.0]]))\n",
    "    target = FlowUnit(np.array([1, 0]))\n",
    "    parameters = (FlowUnit(np.array([0.5, -0.5])), FlowUnit(0.1))\n",
    "\n",
    "    # Test binary_cross_entropy_loss\n",
    "    bce_loss = LossFunctions.binary_cross_entropy_loss(inputs, target, parameters)\n",
    "    print(\"Binary Cross-Entropy Loss:\", bce_loss.data)\n",
    "    bce_loss.backpropagate()\n",
    "    print(\"Binary Cross-Entropy Gradient:\", inputs.grad)\n",
    "    # Test mse_loss\n",
    "    mse_loss = LossFunctions.mse_loss(inputs, target, parameters)\n",
    "    print(\"Mean Squared Error Loss:\", mse_loss.data)\n",
    "    mse_loss.backpropagate()\n",
    "    print(\"Mean Squared Error Gradient:\", inputs.grad)\n",
    "\n",
    "# Run the tests\n",
    "if __name__ == \"__main__\":\n",
    "    test_flow_unit_functions()\n",
    "    test_loss_functions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code implements a simple gradient descent procedure to optimize the parameters (`w1`, `w2`, `b`) of a linear model. Here's a detailed step-by-step explanation of what the code does:\n",
    "\n",
    "### 1. **Input Data and True Output**:\n",
    "```python\n",
    "x = np.array([1.0, 2.0, 3.0])  # Input data\n",
    "y_true = np.array([6.0, 12.0, 18.0])  # Expected output (for simplicity)\n",
    "```\n",
    "- `x` is the input data for the model, and `y_true` is the expected output (for simplicity, it's just a scaled version of `x`).\n",
    "\n",
    "### 2. **Initialize Parameters (`w1`, `w2`, `b`)**:\n",
    "```python\n",
    "w1 = FlowUnit(np.random.randn(1))  # Random initialization for w1\n",
    "w2 = FlowUnit(np.random.randn(1))  # Random initialization for w2\n",
    "b = FlowUnit(np.random.randn(1))   # Random initialization for b\n",
    "```\n",
    "- The parameters `w1`, `w2`, and `b` are initialized with random values. These represent the weights and bias of the linear model. Here, `FlowUnit` is presumably a custom class, likely for handling the forward pass and gradient computation.\n",
    "\n",
    "### 3. **Print Initial Parameter Values**:\n",
    "```python\n",
    "print(f\"The weight 1 value {w1}\")\n",
    "print(f\"The weight 2 value {w2}\")\n",
    "print(f\"The bias start value {b}\")\n",
    "```\n",
    "- The initial values of `w1`, `w2`, and `b` are printed.\n",
    "\n",
    "### 4. **Linear Model Prediction (`y_pred`)**:\n",
    "```python\n",
    "y_pred = w1.data * x + w2.data * x + b.data  # Compute predictions\n",
    "```\n",
    "- The model makes predictions based on the initial parameters. Since this is a simple linear model, the prediction formula is:\n",
    "  \\[\n",
    "  y = w1 \\cdot x + w2 \\cdot x + b\n",
    "  \\]\n",
    "\n",
    "### 5. **Calculate Initial Loss (MSE)**:\n",
    "```python\n",
    "loss = np.mean((y_pred - y_true) ** 2)\n",
    "print(f\"Initial Loss: {loss}\")\n",
    "```\n",
    "- The loss function used is the Mean Squared Error (MSE), which measures how close the predictions are to the true values. The MSE is calculated and printed.\n",
    "\n",
    "### 6. **Manually Calculate Gradients**:\n",
    "```python\n",
    "grad_w1 = 2 * np.mean((y_pred - y_true) * x)  # Gradient with respect to w1\n",
    "grad_w2 = 2 * np.mean((y_pred - y_true) * x)  # Gradient with respect to w2\n",
    "grad_b = 2 * np.mean(y_pred - y_true)         # Gradient with respect to b\n",
    "```\n",
    "- The gradients of the loss with respect to each parameter (`w1`, `w2`, and `b`) are computed manually using the MSE gradient formulas:\n",
    "  - \\( \\frac{\\partial \\text{MSE}}{\\partial w1} = 2 \\cdot \\text{mean}((y_{\\text{pred}} - y_{\\text{true}}) \\cdot x) \\)\n",
    "  - \\( \\frac{\\partial \\text{MSE}}{\\partial w2} = 2 \\cdot \\text{mean}((y_{\\text{pred}} - y_{\\text{true}}) \\cdot x) \\)\n",
    "  - \\( \\frac{\\partial \\text{MSE}}{\\partial b} = 2 \\cdot \\text{mean}(y_{\\text{pred}} - y_{\\text{true}}) \\)\n",
    "\n",
    "### 7. **Print Gradients**:\n",
    "```python\n",
    "print(f\"Gradients: grad_w1: {grad_w1}, grad_w2: {grad_w2}, grad_b: {grad_b}\")\n",
    "```\n",
    "- The gradients of `w1`, `w2`, and `b` are printed.\n",
    "\n",
    "### 8. **Gradient Descent Update**:\n",
    "```python\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Update parameters using gradient descent\n",
    "w1.data -= learning_rate * grad_w1  # Update w1\n",
    "w2.data -= learning_rate * grad_w2  # Update w2\n",
    "b.data -= learning_rate * grad_b    # Update b\n",
    "```\n",
    "- The parameters (`w1`, `w2`, and `b`) are updated using the gradient descent formula:\n",
    "  \\[\n",
    "  \\theta = \\theta - \\text{learning\\_rate} \\cdot \\frac{\\partial \\text{Loss}}{\\partial \\theta}\n",
    "  \\]\n",
    "  where \\(\\theta\\) represents the parameters (`w1`, `w2`, or `b`).\n",
    "\n",
    "### 9. **Print Updated Parameters**:\n",
    "```python\n",
    "print(f\"Updated Parameters after Gradient Descent:\")\n",
    "print(f\"w1: {w1.data}, w2: {w2.data}, b: {b.data}\")\n",
    "```\n",
    "- After applying the gradient descent update, the new values of `w1`, `w2`, and `b` are printed.\n",
    "\n",
    "### 10. **Recompute Predictions and Loss**:\n",
    "```python\n",
    "y_pred_updated = w1.data * x + w2.data * x + b.data  # Updated predictions\n",
    "\n",
    "# Calculate the new loss after the parameter update\n",
    "new_loss = np.mean((y_pred_updated - y_true) ** 2)\n",
    "print(f\"New Loss after update: {new_loss}\")\n",
    "```\n",
    "- The predictions are recomputed with the updated parameters, and the new loss is calculated and printed. This allows you to observe how the loss has improved (or not) after the parameter updates.\n",
    "\n",
    "### Summary:\n",
    "- **Initial Predictions**: Based on the random initialization of the parameters (`w1`, `w2`, `b`).\n",
    "- **Initial Loss**: MSE between the initial predictions and the true values.\n",
    "- **Gradients**: The gradients of the MSE with respect to `w1`, `w2`, and `b`.\n",
    "- **Gradient Descent Update**: The parameters are updated using the gradients computed.\n",
    "- **Updated Loss**: After the gradient descent update, the predictions and the loss are recalculated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weight 1 value FlowUnit(0.6504849091485773)\n",
      "The weight 2 value FlowUnit(0.1492280519827694)\n",
      "The bias start value FlowUnit(-1.297367811312374)\n",
      "Initial Loss: 154.87050129733345\n",
      "Gradients: grad_w1: -53.7254836080236, grad_w2: -53.7254836080236, grad_b: -23.395883778099364\n",
      "Updated Parameters after Gradient Descent:\n",
      "w1: [1.18773975], w2: [0.68648289], b: [-1.06340897]\n",
      "New Loss after update: 98.11657478151464\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1.0, 2.0, 3.0])  # Input data\n",
    "y_true = np.array([6.0, 12.0, 18.0])  # Expected output (for simplicity)\n",
    "\n",
    "# Initialize parameters: w1, w2, and b with random values\n",
    "w1 = FlowUnit(np.random.randn(1))  # Random initialization for w1\n",
    "w2 = FlowUnit(np.random.randn(1))  # Random initialization for w2\n",
    "b = FlowUnit(np.random.randn(1))   # Random initialization for b\n",
    "print(f\"The weight 1 value {w1}\")\n",
    "print(f\"The weight 2 value {w2}\")\n",
    "print(f\"The bias start value {b}\")\n",
    "\n",
    "# Simple linear model: y = w1*x + w2*x + b\n",
    "y_pred = w1.data * x + w2.data * x + b.data  # Compute predictions\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) loss\n",
    "loss = np.mean((y_pred - y_true) ** 2)\n",
    "print(f\"Initial Loss: {loss}\")\n",
    "\n",
    "# Calculate gradients manually (derivatives of MSE with respect to w1, w2, and b)\n",
    "grad_w1 = 2 * np.mean((y_pred - y_true) * x)  # Gradient with respect to w1\n",
    "grad_w2 = 2 * np.mean((y_pred - y_true) * x)  # Gradient with respect to w2\n",
    "grad_b = 2 * np.mean(y_pred - y_true)         # Gradient with respect to b\n",
    "\n",
    "# Print out gradients for w1, w2, and b\n",
    "print(f\"Gradients: grad_w1: {grad_w1}, grad_w2: {grad_w2}, grad_b: {grad_b}\")\n",
    "\n",
    "# Learning rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Update parameters (w1, w2, b) using gradient descent formula\n",
    "w1.data -= learning_rate * grad_w1  # Update w1\n",
    "w2.data -= learning_rate * grad_w2  # Update w2\n",
    "b.data -= learning_rate * grad_b    # Update b\n",
    "\n",
    "# Print updated parameters\n",
    "print(f\"Updated Parameters after Gradient Descent:\")\n",
    "print(f\"w1: {w1.data}, w2: {w2.data}, b: {b.data}\")\n",
    "\n",
    "# Recompute predictions with updated parameters\n",
    "y_pred_updated = w1.data * x + w2.data * x + b.data  # Updated predictions\n",
    "\n",
    "# Calculate the new loss after the parameter update\n",
    "new_loss = np.mean((y_pred_updated - y_true) ** 2)\n",
    "print(f\"New Loss after update: {new_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 1. **Single Neuron Output**:\n",
    "```python\n",
    "neuron = Nuron(3)\n",
    "X = np.array([1.0, 2.0, 3.0])\n",
    "print(\"Neuron Output (without activation):\", neuron(X))\n",
    "```\n",
    "- This part creates a neuron that accepts 3 inputs. The input `X` is an array of size `(3,)`, and the output will depend on how the `Nuron` class computes the weighted sum of these inputs (if implemented this way) and whether any activation function is applied (likely none, given the wording).\n",
    "\n",
    "### 2. **Neural Network Output**:\n",
    "```python\n",
    "neural_network = NeuralNetwork(input_size=3, layers_sizes=[2, 1])\n",
    "print(\"Neural Network Output:\", neural_network(X))\n",
    "```\n",
    "- This part creates a neural network with an input layer of size 3, one hidden layer of size 2, and an output layer of size 1. The `NeuralNetwork` class likely computes the forward pass and applies activations between layers.\n",
    "  \n",
    "You can check if the `NeuralNetwork` and `Nuron` classes properly handle the forward pass and apply necessary operations like weight initialization, activations, and bias terms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron Output (without activation): FlowUnit(2.065210983003632)\n",
      "Neural Network Output: FlowUnit(FlowUnit(1.8525572560203223))\n"
     ]
    }
   ],
   "source": [
    "from Nuron import Nuron\n",
    "from Nuron import Layer\n",
    "from Nuron import NeuralNetwork\n",
    "import numpy as np\n",
    "\n",
    "def test_nuron_neural_network():\n",
    "    # Create a single neuron instance\n",
    "    neuron = Nuron(3)\n",
    "    X = np.array([1.0, 2.0, 3.0])\n",
    "    print(\"Neuron Output (without activation):\", neuron(X))\n",
    "\n",
    "    # Create a neural network instance\n",
    "    neural_network = NeuralNetwork(input_size=3, layers_sizes=[2, 1])\n",
    "    print(\"Neural Network Output:\", neural_network(X))\n",
    "\n",
    "test_nuron_neural_network()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakdown of Code:\n",
    "1. **Creating Neural Network**:\n",
    "   ```python\n",
    "   n = NeuralNetwork(3, [4, 4, 1])  # 3 input neurons, two layers with 4 neurons, and 1 output neuron\n",
    "   ```\n",
    "\n",
    "   This creates a neural network with:\n",
    "   - Input layer of size 3\n",
    "   - Hidden layer 1 with 4 neurons\n",
    "   - Hidden layer 2 with 4 neurons\n",
    "   - Output layer with 1 neuron\n",
    "\n",
    "2. **Layer 1 Output**:\n",
    "   ```python\n",
    "   layer_1_output = n.layers[0](X)\n",
    "   print(\"First Layer Output:\", layer_1_output)\n",
    "   ```\n",
    "\n",
    "   Here, you're passing the input `X` (which is `[2.0, 3.0, -1.0]`) through the first layer of the network, `n.layers[0]`. This layer likely computes a weighted sum of the inputs and applies an activation function.\n",
    "\n",
    "3. **Layer 2 Output**:\n",
    "   ```python\n",
    "   layer_2_output = n.layers[1](layer_1_output)\n",
    "   print(\"Second Layer Output:\", layer_2_output)\n",
    "   ```\n",
    "\n",
    "   Similarly, you're passing the output from the first layer (`layer_1_output`) to the second layer (`n.layers[1]`). The second layer will process the output from the first layer and likely apply another activation function.\n",
    "\n",
    "4. **Final Output**:\n",
    "   ```python\n",
    "   final_output = n.layers[2](layer_2_output)\n",
    "   print(\"Final Output:\", final_output)\n",
    "   ```\n",
    "\n",
    "   Here, you're passing the output of the second layer (`layer_2_output`) to the final output layer (`n.layers[2]`), which will produce the final result after applying a potential activation function.\n",
    "\n",
    "5. **Final Result Using the Entire Network**:\n",
    "   ```python\n",
    "   print(\"The final output\")\n",
    "   n(X)\n",
    "   ```\n",
    "\n",
    "   This line passes the input `X` through the entire network, performing the forward pass through all the layers, which is equivalent to the manual layer-by-layer calculation above.\n",
    "\n",
    "### Output of this Code:\n",
    "- You will see the outputs from each layer printed one by one:\n",
    "  - **First Layer Output**: The result of passing `X` through the first layer.\n",
    "  - **Second Layer Output**: The result of passing the output of the first layer through the second layer.\n",
    "  - **Final Output**: The result of passing the output from the second layer through the final output layer.\n",
    "- Finally, when you run `n(X)`, it will provide the output of the entire neural network for input `X`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Layer Output: [FlowUnit(-0.8827072330279537), FlowUnit(-0.8069609809395097), FlowUnit(-0.6136632631412697), FlowUnit(-0.9350253425896144)]\n",
      "Second Layer Output: [FlowUnit(FlowUnit(-0.6817585172425371)), FlowUnit(FlowUnit(-2.1209023770715283)), FlowUnit(FlowUnit(0.5280491937895537)), FlowUnit(FlowUnit(-0.3149086027994975))]\n",
      "Final Output: FlowUnit(FlowUnit(FlowUnit(-2.4835129313450204)))\n",
      "The final output\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FlowUnit(FlowUnit(FlowUnit(-2.4835129313450204)))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [2.0 , 3.0 , -1.0]\n",
    "n = NeuralNetwork(3 , [4 , 4 , 1]) \n",
    "layer_1_output = n.layers[0](X)\n",
    "print(\"First Layer Output:\", layer_1_output)\n",
    "\n",
    "layer_2_output = n.layers[1](layer_1_output)\n",
    "print(\"Second Layer Output:\", layer_2_output)\n",
    "\n",
    "final_output = n.layers[2](layer_2_output)\n",
    "print(\"Final Output:\", final_output)\n",
    "\n",
    "print(\"The final output\")\n",
    "n(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
